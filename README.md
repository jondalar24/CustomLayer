# üîß Custom Dense Layer en TensorFlow/Keras

Este repositorio contiene una implementaci√≥n desde cero de una **capa densa personalizada** (`CustomDenseLayer`) utilizando la API de subclases de Keras. Esta implementaci√≥n reproduce el comportamiento de `Dense(units, activation='relu')`, permitiendo m√°xima flexibilidad y control sobre la arquitectura del modelo.

---

## üìå ¬øQu√© se implementa?

La clase `CustomDenseLayer` hereda de `tf.keras.layers.Layer` y define los tres m√©todos clave:

### 1. `__init__(self, units)`
- Define el n√∫mero de neuronas (unidades de salida).
- ‚öôÔ∏è **Par√°metros configurables:**
  - `units` *(int)*: n√∫mero de neuronas en la capa.

### 2. `build(self, input_shape)`
- Crea:
  - `self.w`: matriz de pesos con dimensiones `(input_features, units)`
  - `self.b`: vector de sesgos `(units,)`
- ‚öôÔ∏è **Flags posibles en `add_weight`:**
  - `initializer='random_normal'` ‚Üí se puede cambiar por `he_normal`, `glorot_uniform`, etc.
  - `trainable=True` ‚Üí marcar como entrenable o no

### 3. `call(self, inputs)`
- Define la l√≥gica del paso hacia adelante.
- Aplica producto matricial + sesgo + funci√≥n de activaci√≥n:

```python
tf.nn.relu(tf.matmul(inputs, self.w) + self.b)
```
---
### ‚úÖ 2. Construcci√≥n del modelo
```python
model = Sequential([
    CustomDenseLayer(128),
    Dropout(0.5),
    CustomDenseLayer(10),
    Softmax()
])
```
#### Se compone de dos capas densas personalizadas:

- 128 neuronas ocultas con ReLU
- 10 neuronas para salida multiclase
- Dropout(0.5) ayuda a prevenir el sobreajuste
- Softmax() convierte las salidas en probabilidades (clasificaci√≥n multiclase)
---
### ‚úÖ 3. Compilaci√≥n del modelo
```python
model.compile(optimizer='adam',
              loss='categorical_crossentropy',
              metrics=['accuracy'])
```

* adam: optimizador adaptativo muy usado
* categorical_crossentropy: apropiado para clasificaci√≥n multiclase con etiquetas one-hot
---
### ‚úÖ 4. Datos sint√©ticos para entrenamiento
```python
x_train = np.random.random((1000, 20))
y_train = np.random.randint(10, size=(1000, 1))
y_train = tf.keras.utils.to_categorical(y_train, num_classes=10)

x_train: 1000 muestras con 20 variables de entrada

y_train: 10 clases posibles (0 a 9), convertidas a one-hot
```
#### üìå Puedes reemplazar estos datos por un dataset real para observar un aprendizaje significativo.
---
### ‚úÖ 5. Entrenamiento y evaluaci√≥n
```python
model.fit(x_train, y_train, epochs=10, batch_size=32)
model.evaluate(x_test, y_test)
```
- Entrena durante 10 √©pocas con batches de 32 muestras
- Eval√∫a la p√©rdida y precisi√≥n sobre un conjunto de test tambi√©n aleatorio
---
## ‚öôÔ∏è Par√°metros que puedes modificar

| Par√°metro       | Lugar en el c√≥digo          | Valores posibles / efecto                                       |
|-----------------|-----------------------------|------------------------------------------------------------------|
| `units`         | `CustomDenseLayer(units)`   | N√∫mero de neuronas por capa                                     |
| `activation`    | En `call()`                 | `tf.nn.relu`, `tanh`, `sigmoid`, etc.                           |
| `initializer`   | En `add_weight()`           | `'he_normal'`, `'glorot_uniform'`, `'random_normal'`, etc.      |
| `Dropout rate`  | `Dropout(0.5)`              | Valor entre 0 y 1                                                |
| `loss`          | `model.compile()`           | `'categorical_crossentropy'`, `'sparse_categorical_crossentropy'` |
| `optimizer`     | `adam`, `sgd`, `rmsprop`, etc. | Afecta c√≥mo se ajustan los pesos durante el entrenamiento     |

---
#### üß™ Ejecuci√≥n
Este script puede ejecutarse directamente en un entorno con TensorFlow 2.x:
```bash
python custom_dense_model.py
```
---
### üìö Aplicaciones
Esta t√©cnica de crear capas personalizadas es √∫til cuando:

- Quieres experimentar con nuevas arquitecturas
- Necesitas a√±adir l√≥gica espec√≠fica no disponible en las capas est√°ndar
- Est√°s trabajando en investigaci√≥n o pruebas de concepto

## üß† Cr√©ditos y contexto
Esta pr√°ctica tiene como objetivo afianzar el uso de la API de subclases para crear capas reutilizables, entendiendo a fondo c√≥mo funcionan internamente las redes neuronales.
